# i18n Translation Services (CTranslate2 backend)
#
# CPU (default):
#   docker compose -f docker/docker-compose.i18n.yml up -d
#
# GPU (NVIDIA):
#   docker compose -f docker/docker-compose.i18n.yml --profile gpu up -d
#
# Multi-dev (LAN) mode:
#   NLLB_NETWORK_MODE=lan docker compose -f docker/docker-compose.i18n.yml up -d
#
# Model conversion (one-time, before first run):
#   docker compose -f docker/docker-compose.i18n.yml run --rm nllb-converter \
#     facebook/nllb-200-distilled-600M int8_float16
#
# The GPU profile reserves the GPU device. Only one of nllb (CPU) or
# nllb-gpu runs — they share port 8000.
#
# CT2 models are persisted in .docker-volumes/nllb-ct2-models/ (gitignored).
# First run triggers automatic HF→CT2 conversion if models aren't present.
#
# Security:
#   - TLS with auto-generated self-signed cert (ephemeral tmpfs)
#   - HMAC-SHA256 bearer token auth on /translate
#   - Localhost-only port binding by default (opt-in LAN via NLLB_NETWORK_MODE)
#   - API key file mounted read-only
#   - All default capabilities dropped; only NET_BIND_SERVICE re-added
#   - Read-only root filesystem (tmpfs for /tmp)
#   - No privilege escalation
#   - Non-root user (nllb) inside container
#   - No access to Docker socket or host network
#   - pip-audit run at build time — build fails on any CVE

x-nllb-common: &nllb-common
  ports:
    - "${NLLB_BIND:-127.0.0.1}:8000:8000"
  environment:
    # Model + compute type auto-selected by server using CT2 APIs.
    # Override with NLLB_PARAMS (600M/1.3B/3.3B), NLLB_PRECISION, NLLB_DEVICE.
    # MODEL_NAME is advanced — prefer NLLB_PARAMS for friendly aliases.
    - MODEL_NAME=${MODEL_NAME:-}
    - NLLB_PARAMS=${NLLB_PARAMS:-}
    - NLLB_PRECISION=${NLLB_PRECISION:-}
    - NLLB_DEVICE=${NLLB_DEVICE:-}
    - HF_HOME=/home/nllb/.cache/huggingface
    - NLLB_CT2_MODEL_DIR=/data/ct2-models
    - NLLB_API_KEY=${NLLB_API_KEY:-}
    # Resource monitor thresholds (see nllb-server.py ResourceMonitor)
    - NLLB_VRAM_SOFT_MB=${NLLB_VRAM_SOFT_MB:-2000}
    - NLLB_VRAM_HARD_MB=${NLLB_VRAM_HARD_MB:-500}
    - NLLB_RAM_SOFT_MB=${NLLB_RAM_SOFT_MB:-4000}
    - NLLB_RAM_HARD_MB=${NLLB_RAM_HARD_MB:-1000}
    - NLLB_SWAP_HARD_MB=${NLLB_SWAP_HARD_MB:-0}
    - NLLB_MONITOR_INTERVAL_S=${NLLB_MONITOR_INTERVAL_S:-5.0}
    - NLLB_MONITOR_FAST_INTERVAL_S=${NLLB_MONITOR_FAST_INTERVAL_S:-0.25}
    - NLLB_MONITOR_LOG_INTERVAL_S=${NLLB_MONITOR_LOG_INTERVAL_S:-30.0}
  volumes:
    - ../.docker-volumes/nllb-cache:/home/nllb/.cache:rw
    - ../.docker-volumes/nllb-ct2-models:/data/ct2-models:rw
    - ../.docker-volumes/nllb-api-key:/run/secrets/nllb-api-key:ro
    - ../.docker-volumes/nllb-data:/data:rw
  restart: unless-stopped
  read_only: true
  tmpfs:
    - /tmp:size=256m
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  cap_add:
    - NET_BIND_SERVICE
  networks:
    - i18n-internal
  healthcheck:
    test:
      [
        "CMD",
        "python",
        "-c",
        "import ssl; import urllib.request; ctx = ssl.create_default_context(); ctx.check_hostname = False; ctx.verify_mode = ssl.CERT_NONE; urllib.request.urlopen('https://localhost:8000/health', context=ctx)",
      ]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 120s

services:
  # ── Model converter (one-time utility) ─────────────────────────────
  nllb-converter:
    build:
      context: ..
      dockerfile: docker/Dockerfile.nllb
      target: converter
    volumes:
      - ../.docker-volumes/nllb-ct2-models:/data/ct2-models:rw
      - ../.docker-volumes/nllb-cache:/root/.cache:rw
    profiles: ["converter"]

  # ── CPU variant (default) ────────────────────────────────────────
  nllb:
    <<: *nllb-common
    build:
      context: ..
      dockerfile: docker/Dockerfile.nllb
      target: runtime
      args:
        GIT_COMMIT_DATE: ${GIT_COMMIT_DATE:-}
    profiles: ["", "cpu"]

  # ── GPU variant (--profile gpu) ──────────────────────────────────
  nllb-gpu:
    <<: *nllb-common
    build:
      context: ..
      dockerfile: docker/Dockerfile.nllb
      target: runtime
      args:
        GIT_COMMIT_DATE: ${GIT_COMMIT_DATE:-}
    profiles: ["gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  i18n-internal:
    driver: bridge
    internal: false # needs outbound for HuggingFace model download on first run
