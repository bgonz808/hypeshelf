# NLLB Translation Server
#
# Serves Facebook's NLLB-200 model via FastAPI with auto-precision selection.
#
# Build args:
#   TORCH_VARIANT  - cpu (default, ~200 MB) or cu128 (CUDA 12.8, ~2 GB)
#
# Precision is auto-detected at runtime:
#   CPU:  fp32
#   GPU:  bf16 (Ampere+) or fp16, with int8/int4 fallback for small VRAM
#
# Security: runs as non-root user, minimal filesystem writes.
# Model is downloaded on first run and cached via bind mount.

FROM python:3.14-slim

WORKDIR /app

ARG TORCH_VARIANT=cpu

# System deps for sentencepiece (build-essential needed for compile, removed after)
RUN apt-get update && \
    apt-get install -y --no-install-recommends build-essential && \
    rm -rf /var/lib/apt/lists/*

# Torch — installed first with variant-specific index
RUN pip install --no-cache-dir \
    torch==2.10.0 --index-url https://download.pytorch.org/whl/${TORCH_VARIANT}

# Remaining Python deps (standard PyPI)
RUN pip install --no-cache-dir \
    transformers==5.0.0 \
    sentencepiece==0.2.1 \
    accelerate==1.6.0 \
    fastapi==0.128.0 \
    uvicorn==0.40.0 \
    cryptography==46.0.4

# bitsandbytes — multi-backend int8/int4 quantization (~59 MB)
# Supports CUDA, AMD ROCm, Apple MPS, and CPU (AVX512-optimized)
RUN pip install --no-cache-dir bitsandbytes==0.49.1

# Remove build tools (no longer needed at runtime)
RUN apt-get purge -y --auto-remove build-essential && \
    rm -rf /var/lib/apt/lists/*

# Bake git commit date into the image so /health can report version_at
# without needing .git in the container. Pass at build time:
#   docker build --build-arg GIT_COMMIT_DATE=$(git log -1 --format=%aI -- docker/nllb-server.py)
ARG GIT_COMMIT_DATE=""
ENV NLLB_GIT_COMMIT_DATE=${GIT_COMMIT_DATE}

COPY docker/nllb-server.py /app/server.py

# Non-root user for runtime
RUN groupadd -r nllb && useradd -r -g nllb -d /home/nllb -m nllb
# Pre-create cache dir owned by nllb user
RUN mkdir -p /home/nllb/.cache && chown -R nllb:nllb /home/nllb

USER nllb

# Model auto-selected by server at runtime. Override with NLLB_PARAMS or MODEL_NAME.
ENV MODEL_NAME=""
ENV HOST="0.0.0.0"
ENV PORT="8000"
ENV HF_HOME="/home/nllb/.cache/huggingface"

EXPOSE 8000

CMD ["python", "server.py"]
